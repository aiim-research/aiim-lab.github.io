<p>When a bias impacts human beings as individuals or as groups characterized by certain
legally-protected sensitive attributes (e.g., gender), the inequalities reinforced by search and
recommendation algorithms can lead to severe societal consequences, such as discrimination and
unfairness. Considering also that the University of L’Aquila (UAQ) has adopted the “Gender
equality plan” approved by the “Board of Directors” on 15/12/2021. We proposed FAIR-EDU:
Promote FAIRness in EDUcation Institutions, a project of six months where we test and estimate
the algorithmic bias present in the staff-related data generated and used by UAQ. We identified
seven research questions that will be answered in four interleaved phases carried on with the
support of the GEP’s WG and the IT staff of UAQ.</p>

<h2 id="introduction">Introduction</h2>

<p>Both search and recommendation algorithms rank users with results that aim to match their needs
and interests. Despite the (non-)personalized perspective that characterizes each class of
algorithms, both learn patterns from data which often conveys biases regarding unbalances and
inequalities.</p>

<p>In most cases, the trained models and, by extension, the final ranking capture and strengthen
these biases in the learned patterns. When a bias impacts human beings as individuals or groups
characterized by certain legally-protected sensitive attributes (e.g., their race, gender, or religion),
the inequalities reinforced by search and recommendation algorithms even lead to severe societal
consequences, such as discrimination and unfairness [1].</p>

<p>Detecting, characterizing, and mitigating biases while preserving effectiveness is thus a timely goal
for modern search and recommendation algorithms that are becoming central in several
application domains. Challenges that arise in real-world applications (e.g., justice [2], education [4],
etc.) are focused on controlling the effects of popularity biases to improve users’ perceived quality
of the results.</p>

<p>Different methods have been proposed to mitigate bias at several levels of data processing, mostly
focusing on classification problems. However, it must be noticed that the multi-class classification
problem is still not effectively addressed, even if it is widely adopted and constitutes a building
block for personalization and search systems in critical domains [5-7].</p>

<p>Historically, the research community of Classic Academic Systems (CAS) was mainly focused on
evaluating the bias in students-related tasks, such as their enrollment and the evaluation of their
learning [8-10], rather than considering and assessing the bias related to the academic’s staff
(e.g., researchers, professors, technicians, administrative, etc.).
Moreover, in the past year, the University of L’Aquila (UAQ) has approved the “Gender equality
plan” approved by the “Board of Directors” with resolution no. 389 of 15/12/2021 “.
The Gender Equality Plan of UAQ is part of a path already started and now consolidated for the
promotion of gender equality and the achievement of objectives of equality, participation, and
non-discrimination.</p>

<h2 id="planned-objectives">Planned Objectives</h2>
<p>Considering the problems and scenarios in FAIR-EDU: Promote FAIRness in EDUcation
Institutions project, we analyzed the algorithmic bias present in the staff-related data generated
and used by the University of L’Aquila.
We remark that the project aims to be an exploratory path that constitutes the basis for further
development at the National and European levels.
Hereafter we report the identified research questions to clarify later which are the specific
deliverable made by the project that answers them:</p>
<ul>
  <li>RQ1: Which are the typical scenarios where the algorithmic bias is present in CAS?</li>
  <li>RQ2: How is the algorithmic bias typically measured in CAS?</li>
  <li>RQ3: Which are the best methods to mitigate algorithmic bias in CAS?</li>
  <li>RQ4: Which UAQ data can be subjected to algorithmic bias?</li>
  <li>RQ5: What is the level of algorithmic bias present in the data of UAQ?</li>
  <li>RQ6: Which are the best methods to mitigate algorithmic bias in the data of UAQ?</li>
  <li>RQ7: How the learned experience can be integrated at local (UAQ) and National Levels?</li>
</ul>

<h2 id="final-remarks">Final Remarks</h2>

<p>FAIR-EDU is focused on analyzing the algorithmic bias in the data of the University of L’Aquila
thus, its contribution to the gender issue is twofold:</p>
<ul>
  <li>in the literature, one of the sensible variables that are related to the algorithmic bias
problem is the gender one. The pilot that we conducted in the FAIR-EDU project was
focused on gender, and thus it will contribute to a better understanding of it at the University
of L’Aquila;</li>
  <li>the outcome of the project will be available and will be discussed within the working group
of the Gender Equality Plan constituting a possible base to highlight further concrete
actions able to improve the University policies and reduce any discrimination.
Three main points can summarise the main advancement achieved by this project:</li>
  <li>FAIR-EDU is focused mainly on the multi-class classification problem, which was not
effectively addressed, even if it is widely adopted and constitutes a building block for
personalization and search systems in several domains;</li>
  <li>the research community is mainly focused on evaluating the bias on students oriented
tasks such as their enrollment and the evaluation of the learning; instead, FAIR-EDU is
focused on considering and assessing the bias related to the academic’s staff (e.g.,
researchers, professors, technicians, administrative, etc.).</li>
  <li>no project until now was specifically tailored to the Italian Classical Academic Systems;
instead, FAIR-EDU is focused on the University of L’Aquila;</li>
</ul>
